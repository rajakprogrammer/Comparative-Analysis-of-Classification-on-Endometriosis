{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed and organized into train, val, and test folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "data_dir = r'D:\\7th sem\\computer vision\\project\\histopathological image dataset for ET\\histopathological image dataset for ET'  # Replace with the correct path\n",
    "output_dir = r'D:\\7th sem\\computer vision\\project\\histopathological image dataset for ET\\split_dataset'\n",
    "\n",
    "# Create base directories for train, val, and test\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/train', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/val', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/test', exist_ok=True)\n",
    "\n",
    "# Process each folder, treating each subfolder as its own class\n",
    "for main_class_name in os.listdir(data_dir):\n",
    "    main_class_path = os.path.join(data_dir, main_class_name)\n",
    "    \n",
    "    if os.path.isdir(main_class_path):\n",
    "        # Check for subdirectories within the main class folder\n",
    "        subdirs = [d for d in os.listdir(main_class_path) if os.path.isdir(os.path.join(main_class_path, d))]\n",
    "        \n",
    "        if subdirs:\n",
    "            # If subfolders exist, treat each subfolder as a distinct class\n",
    "            for subdir in subdirs:\n",
    "                subdir_path = os.path.join(main_class_path, subdir)\n",
    "                class_name = f\"{main_class_name}_{subdir}\"  # Unique class name, e.g., \"NE_Follicular\"\n",
    "                images = os.listdir(subdir_path)\n",
    "\n",
    "                # Skip if there are not enough images to split\n",
    "                if len(images) < 4:\n",
    "                    print(f\"Skipping class '{class_name}' as it has fewer than 4 images.\")\n",
    "                    continue\n",
    "\n",
    "                # Split into train (70%), val (15%), and test (15%)\n",
    "                train_images, temp_images = train_test_split(images, test_size=0.3, random_state=42)\n",
    "                if len(temp_images) >= 2:\n",
    "                    val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n",
    "                else:\n",
    "                    val_images, test_images = temp_images, []\n",
    "\n",
    "                # Create class directories within train, val, and test folders\n",
    "                for split in ['train', 'val', 'test']:\n",
    "                    os.makedirs(f'{output_dir}/{split}/{class_name}', exist_ok=True)\n",
    "\n",
    "                # Copy images to the respective split folders\n",
    "                for image in train_images:\n",
    "                    shutil.copy(os.path.join(subdir_path, image), f'{output_dir}/train/{class_name}/{image}')\n",
    "                \n",
    "                for image in val_images:\n",
    "                    shutil.copy(os.path.join(subdir_path, image), f'{output_dir}/val/{class_name}/{image}')\n",
    "                \n",
    "                for image in test_images:\n",
    "                    shutil.copy(os.path.join(subdir_path, image), f'{output_dir}/test/{class_name}/{image}')\n",
    "        else:\n",
    "            # No subfolders; treat main folder as a single class\n",
    "            class_name = main_class_name\n",
    "            images = os.listdir(main_class_path)\n",
    "\n",
    "            # Skip if there are not enough images to split\n",
    "            if len(images) < 4:\n",
    "                print(f\"Skipping class '{class_name}' as it has fewer than 4 images.\")\n",
    "                continue\n",
    "\n",
    "            # Split into train (70%), val (15%), and test (15%)\n",
    "            train_images, temp_images = train_test_split(images, test_size=0.3, random_state=42)\n",
    "            if len(temp_images) >= 2:\n",
    "                val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n",
    "            else:\n",
    "                val_images, test_images = temp_images, []\n",
    "\n",
    "            # Create class directories within train, val, and test folders\n",
    "            for split in ['train', 'val', 'test']:\n",
    "                os.makedirs(f'{output_dir}/{split}/{class_name}', exist_ok=True)\n",
    "\n",
    "            # Copy images to the respective split folders\n",
    "            for image in train_images:\n",
    "                shutil.copy(os.path.join(main_class_path, image), f'{output_dir}/train/{class_name}/{image}')\n",
    "            \n",
    "            for image in val_images:\n",
    "                shutil.copy(os.path.join(main_class_path, image), f'{output_dir}/val/{class_name}/{image}')\n",
    "            \n",
    "            for image in test_images:\n",
    "                shutil.copy(os.path.join(main_class_path, image), f'{output_dir}/test/{class_name}/{image}')\n",
    "\n",
    "print(\"Dataset split completed and organized into train, val, and test folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanjay baitha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6\n",
      "Class names: ['EA', 'EH_Complex', 'EH_Simple', 'EP', 'NE_Follicular', 'NE_Luteal']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define paths to the train, validation, and test directories\n",
    "train_dir = r'D:\\Engineering\\SEM7\\COMPUTER VISION\\proj_imp\\DATASET\\org_datahistopath_img_data\\train'\n",
    "val_dir = r'D:\\Engineering\\SEM7\\COMPUTER VISION\\proj_imp\\DATASET\\org_datahistopath_img_data\\val'\n",
    "test_dir = r'D:\\Engineering\\SEM7\\COMPUTER VISION\\proj_imp\\DATASET\\org_datahistopath_img_data\\test'\n",
    "\n",
    "# Define image transformations (resize, convert to tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resizing to fit model input (e.g., ResNet50)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Create data loaders for batching and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check the number of classes and class names\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Class names:\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Load the pretrained model\n",
    "model = models.vgg16(weights='VGG16_Weights.DEFAULT')\n",
    "\n",
    "# Modify the last layer to match the number of classes (7 in this case)\n",
    "num_classes = 7\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "\n",
    "# Move model to the correct device (if using GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n",
      "Epoch [1/100], Loss: 1.2003, Accuracy: 49.56%\n",
      "Validation Loss: 1.0873, Validation Accuracy: 59.15%\n",
      "\n",
      "Epoch [2/100], Loss: 0.8609, Accuracy: 65.52%\n",
      "Validation Loss: 0.8200, Validation Accuracy: 65.85%\n",
      "\n",
      "Epoch [3/100], Loss: 0.6357, Accuracy: 75.20%\n",
      "Validation Loss: 0.8156, Validation Accuracy: 68.50%\n",
      "\n",
      "Epoch [4/100], Loss: 0.4904, Accuracy: 81.60%\n",
      "Validation Loss: 1.0148, Validation Accuracy: 62.80%\n",
      "\n",
      "Epoch [5/100], Loss: 0.3536, Accuracy: 86.92%\n",
      "Validation Loss: 1.3238, Validation Accuracy: 58.54%\n",
      "\n",
      "Epoch [6/100], Loss: 0.2174, Accuracy: 92.15%\n",
      "Validation Loss: 1.1572, Validation Accuracy: 70.33%\n",
      "\n",
      "Epoch [7/100], Loss: 0.1079, Accuracy: 96.34%\n",
      "Validation Loss: 1.4236, Validation Accuracy: 63.82%\n",
      "\n",
      "Epoch [8/100], Loss: 0.1330, Accuracy: 96.03%\n",
      "Validation Loss: 1.2894, Validation Accuracy: 62.40%\n",
      "\n",
      "Epoch [9/100], Loss: 0.1176, Accuracy: 95.42%\n",
      "Validation Loss: 1.2422, Validation Accuracy: 71.75%\n",
      "\n",
      "Epoch [10/100], Loss: 0.0968, Accuracy: 96.56%\n",
      "Validation Loss: 1.4510, Validation Accuracy: 66.46%\n",
      "\n",
      "Epoch [11/100], Loss: 0.0844, Accuracy: 97.04%\n",
      "Validation Loss: 1.2463, Validation Accuracy: 68.70%\n",
      "\n",
      "Epoch [12/100], Loss: 0.0407, Accuracy: 98.78%\n",
      "Validation Loss: 1.6032, Validation Accuracy: 70.53%\n",
      "\n",
      "Epoch [13/100], Loss: 0.0374, Accuracy: 98.47%\n",
      "Validation Loss: 1.7172, Validation Accuracy: 68.50%\n",
      "\n",
      "Epoch [14/100], Loss: 0.0996, Accuracy: 96.64%\n",
      "Validation Loss: 1.5663, Validation Accuracy: 65.04%\n",
      "\n",
      "Epoch [15/100], Loss: 0.0539, Accuracy: 98.30%\n",
      "Validation Loss: 1.8910, Validation Accuracy: 68.09%\n",
      "\n",
      "Epoch [16/100], Loss: 0.0335, Accuracy: 98.82%\n",
      "Validation Loss: 1.7599, Validation Accuracy: 69.51%\n",
      "\n",
      "Epoch [17/100], Loss: 0.0142, Accuracy: 99.69%\n",
      "Validation Loss: 1.5805, Validation Accuracy: 70.12%\n",
      "\n",
      "Epoch [18/100], Loss: 0.0076, Accuracy: 99.61%\n",
      "Validation Loss: 1.5772, Validation Accuracy: 72.15%\n",
      "\n",
      "Epoch [19/100], Loss: 0.0181, Accuracy: 99.26%\n",
      "Validation Loss: 1.7133, Validation Accuracy: 68.70%\n",
      "\n",
      "Epoch [20/100], Loss: 0.0703, Accuracy: 97.52%\n",
      "Validation Loss: 1.7348, Validation Accuracy: 68.70%\n",
      "\n",
      "Epoch [21/100], Loss: 0.0773, Accuracy: 97.21%\n",
      "Validation Loss: 1.9828, Validation Accuracy: 61.59%\n",
      "\n",
      "Epoch [22/100], Loss: 0.1392, Accuracy: 95.55%\n",
      "Validation Loss: 1.3604, Validation Accuracy: 66.06%\n",
      "\n",
      "Epoch [23/100], Loss: 0.0499, Accuracy: 97.78%\n",
      "Validation Loss: 2.2405, Validation Accuracy: 65.24%\n",
      "\n",
      "Epoch [24/100], Loss: 0.1286, Accuracy: 95.99%\n",
      "Validation Loss: 2.0374, Validation Accuracy: 67.89%\n",
      "\n",
      "Epoch [25/100], Loss: 0.0602, Accuracy: 98.13%\n",
      "Validation Loss: 1.5804, Validation Accuracy: 69.31%\n",
      "\n",
      "Epoch [26/100], Loss: 0.0125, Accuracy: 99.30%\n",
      "Validation Loss: 1.6395, Validation Accuracy: 69.31%\n",
      "\n",
      "Epoch [27/100], Loss: 0.0401, Accuracy: 98.56%\n",
      "Validation Loss: 1.6691, Validation Accuracy: 63.62%\n",
      "\n",
      "Epoch [28/100], Loss: 0.0315, Accuracy: 98.61%\n",
      "Validation Loss: 1.7380, Validation Accuracy: 65.65%\n",
      "\n",
      "Epoch [29/100], Loss: 0.0907, Accuracy: 96.99%\n",
      "Validation Loss: 1.7675, Validation Accuracy: 64.23%\n",
      "\n",
      "Epoch [30/100], Loss: 0.0215, Accuracy: 99.17%\n",
      "Validation Loss: 2.0685, Validation Accuracy: 65.04%\n",
      "\n",
      "Epoch [31/100], Loss: 0.0453, Accuracy: 98.61%\n",
      "Validation Loss: 1.6192, Validation Accuracy: 69.11%\n",
      "\n",
      "Epoch [32/100], Loss: 0.0511, Accuracy: 98.30%\n",
      "Validation Loss: 1.5507, Validation Accuracy: 64.84%\n",
      "\n",
      "Epoch [33/100], Loss: 0.0252, Accuracy: 99.08%\n",
      "Validation Loss: 1.9049, Validation Accuracy: 64.43%\n",
      "\n",
      "Epoch [34/100], Loss: 0.0082, Accuracy: 99.56%\n",
      "Validation Loss: 1.8733, Validation Accuracy: 66.87%\n",
      "\n",
      "Epoch [35/100], Loss: 0.0058, Accuracy: 99.56%\n",
      "Validation Loss: 1.8943, Validation Accuracy: 66.67%\n",
      "\n",
      "Epoch [36/100], Loss: 0.0057, Accuracy: 99.52%\n",
      "Validation Loss: 1.9398, Validation Accuracy: 67.48%\n",
      "\n",
      "Epoch [37/100], Loss: 0.0062, Accuracy: 99.52%\n",
      "Validation Loss: 1.8681, Validation Accuracy: 66.87%\n",
      "\n",
      "Epoch [38/100], Loss: 0.0052, Accuracy: 99.61%\n",
      "Validation Loss: 1.9157, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [39/100], Loss: 0.0057, Accuracy: 99.48%\n",
      "Validation Loss: 1.9353, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [40/100], Loss: 0.0051, Accuracy: 99.61%\n",
      "Validation Loss: 1.9670, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [41/100], Loss: 0.0056, Accuracy: 99.48%\n",
      "Validation Loss: 1.9394, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [42/100], Loss: 0.0056, Accuracy: 99.56%\n",
      "Validation Loss: 1.9222, Validation Accuracy: 66.06%\n",
      "\n",
      "Epoch [43/100], Loss: 0.0058, Accuracy: 99.56%\n",
      "Validation Loss: 1.9461, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [44/100], Loss: 0.0053, Accuracy: 99.48%\n",
      "Validation Loss: 1.9599, Validation Accuracy: 67.48%\n",
      "\n",
      "Epoch [45/100], Loss: 0.0052, Accuracy: 99.43%\n",
      "Validation Loss: 1.9786, Validation Accuracy: 67.68%\n",
      "\n",
      "Epoch [46/100], Loss: 0.0051, Accuracy: 99.56%\n",
      "Validation Loss: 2.0167, Validation Accuracy: 66.67%\n",
      "\n",
      "Epoch [47/100], Loss: 0.0048, Accuracy: 99.69%\n",
      "Validation Loss: 2.1094, Validation Accuracy: 67.48%\n",
      "\n",
      "Epoch [48/100], Loss: 0.0048, Accuracy: 99.65%\n",
      "Validation Loss: 2.1009, Validation Accuracy: 67.48%\n",
      "\n",
      "Epoch [49/100], Loss: 0.0048, Accuracy: 99.61%\n",
      "Validation Loss: 2.0872, Validation Accuracy: 67.68%\n",
      "\n",
      "Epoch [50/100], Loss: 0.0054, Accuracy: 99.56%\n",
      "Validation Loss: 2.0855, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [51/100], Loss: 0.0051, Accuracy: 99.61%\n",
      "Validation Loss: 2.0916, Validation Accuracy: 67.48%\n",
      "\n",
      "Epoch [52/100], Loss: 0.0049, Accuracy: 99.52%\n",
      "Validation Loss: 2.0925, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [53/100], Loss: 0.0048, Accuracy: 99.56%\n",
      "Validation Loss: 2.1433, Validation Accuracy: 67.89%\n",
      "\n",
      "Epoch [54/100], Loss: 0.0053, Accuracy: 99.65%\n",
      "Validation Loss: 2.0714, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [55/100], Loss: 0.0047, Accuracy: 99.56%\n",
      "Validation Loss: 2.1391, Validation Accuracy: 67.68%\n",
      "\n",
      "Epoch [56/100], Loss: 0.0052, Accuracy: 99.61%\n",
      "Validation Loss: 2.1488, Validation Accuracy: 67.68%\n",
      "\n",
      "Epoch [57/100], Loss: 0.0047, Accuracy: 99.61%\n",
      "Validation Loss: 2.1143, Validation Accuracy: 68.09%\n",
      "\n",
      "Epoch [58/100], Loss: 0.0054, Accuracy: 99.52%\n",
      "Validation Loss: 2.1079, Validation Accuracy: 67.48%\n",
      "\n",
      "Epoch [59/100], Loss: 0.0051, Accuracy: 99.52%\n",
      "Validation Loss: 2.1111, Validation Accuracy: 68.09%\n",
      "\n",
      "Epoch [60/100], Loss: 0.0047, Accuracy: 99.69%\n",
      "Validation Loss: 2.2065, Validation Accuracy: 68.09%\n",
      "\n",
      "Epoch [61/100], Loss: 0.0054, Accuracy: 99.56%\n",
      "Validation Loss: 2.1084, Validation Accuracy: 66.87%\n",
      "\n",
      "Epoch [62/100], Loss: 0.0045, Accuracy: 99.74%\n",
      "Validation Loss: 2.2548, Validation Accuracy: 67.89%\n",
      "\n",
      "Epoch [63/100], Loss: 0.0054, Accuracy: 99.61%\n",
      "Validation Loss: 2.1344, Validation Accuracy: 68.09%\n",
      "\n",
      "Epoch [64/100], Loss: 0.0049, Accuracy: 99.52%\n",
      "Validation Loss: 2.2170, Validation Accuracy: 67.89%\n",
      "\n",
      "Epoch [65/100], Loss: 0.0051, Accuracy: 99.61%\n",
      "Validation Loss: 2.2627, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [66/100], Loss: 0.0053, Accuracy: 99.56%\n",
      "Validation Loss: 2.1545, Validation Accuracy: 67.68%\n",
      "\n",
      "Epoch [67/100], Loss: 0.0045, Accuracy: 99.78%\n",
      "Validation Loss: 2.2629, Validation Accuracy: 68.70%\n",
      "\n",
      "Epoch [68/100], Loss: 0.0048, Accuracy: 99.65%\n",
      "Validation Loss: 2.2265, Validation Accuracy: 68.50%\n",
      "\n",
      "Epoch [69/100], Loss: 0.0047, Accuracy: 99.56%\n",
      "Validation Loss: 2.3126, Validation Accuracy: 68.90%\n",
      "\n",
      "Epoch [70/100], Loss: 0.0055, Accuracy: 99.56%\n",
      "Validation Loss: 2.3233, Validation Accuracy: 68.90%\n",
      "\n",
      "Epoch [71/100], Loss: 0.4798, Accuracy: 83.78%\n",
      "Validation Loss: 1.0726, Validation Accuracy: 65.45%\n",
      "\n",
      "Epoch [72/100], Loss: 0.3607, Accuracy: 88.27%\n",
      "Validation Loss: 1.3535, Validation Accuracy: 64.02%\n",
      "\n",
      "Epoch [73/100], Loss: 0.1492, Accuracy: 95.07%\n",
      "Validation Loss: 1.7933, Validation Accuracy: 56.91%\n",
      "\n",
      "Epoch [74/100], Loss: 0.1987, Accuracy: 93.03%\n",
      "Validation Loss: 1.3459, Validation Accuracy: 61.59%\n",
      "\n",
      "Epoch [75/100], Loss: 0.0556, Accuracy: 98.04%\n",
      "Validation Loss: 1.7577, Validation Accuracy: 61.79%\n",
      "\n",
      "Epoch [76/100], Loss: 0.0342, Accuracy: 98.61%\n",
      "Validation Loss: 1.7321, Validation Accuracy: 65.65%\n",
      "\n",
      "Epoch [77/100], Loss: 0.0091, Accuracy: 99.52%\n",
      "Validation Loss: 1.7754, Validation Accuracy: 64.63%\n",
      "\n",
      "Epoch [78/100], Loss: 0.0058, Accuracy: 99.65%\n",
      "Validation Loss: 1.8359, Validation Accuracy: 66.67%\n",
      "\n",
      "Epoch [79/100], Loss: 0.0058, Accuracy: 99.48%\n",
      "Validation Loss: 1.8371, Validation Accuracy: 66.87%\n",
      "\n",
      "Epoch [80/100], Loss: 0.0054, Accuracy: 99.61%\n",
      "Validation Loss: 1.8413, Validation Accuracy: 66.67%\n",
      "\n",
      "Epoch [81/100], Loss: 0.0053, Accuracy: 99.61%\n",
      "Validation Loss: 1.8660, Validation Accuracy: 66.06%\n",
      "\n",
      "Epoch [82/100], Loss: 0.0053, Accuracy: 99.61%\n",
      "Validation Loss: 1.8714, Validation Accuracy: 66.67%\n",
      "\n",
      "Epoch [83/100], Loss: 0.0050, Accuracy: 99.61%\n",
      "Validation Loss: 1.8790, Validation Accuracy: 67.89%\n",
      "\n",
      "Epoch [84/100], Loss: 0.0053, Accuracy: 99.52%\n",
      "Validation Loss: 1.8903, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [85/100], Loss: 0.0054, Accuracy: 99.52%\n",
      "Validation Loss: 1.8925, Validation Accuracy: 66.87%\n",
      "\n",
      "Epoch [86/100], Loss: 0.0047, Accuracy: 99.65%\n",
      "Validation Loss: 1.9330, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [87/100], Loss: 0.0053, Accuracy: 99.56%\n",
      "Validation Loss: 1.9428, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [88/100], Loss: 0.0049, Accuracy: 99.61%\n",
      "Validation Loss: 1.9518, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [89/100], Loss: 0.0044, Accuracy: 99.74%\n",
      "Validation Loss: 1.9885, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [90/100], Loss: 0.0051, Accuracy: 99.61%\n",
      "Validation Loss: 2.0062, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [91/100], Loss: 0.0049, Accuracy: 99.61%\n",
      "Validation Loss: 1.9752, Validation Accuracy: 67.07%\n",
      "\n",
      "Epoch [92/100], Loss: 0.0045, Accuracy: 99.65%\n",
      "Validation Loss: 2.0298, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [93/100], Loss: 0.0048, Accuracy: 99.65%\n",
      "Validation Loss: 2.0330, Validation Accuracy: 67.48%\n",
      "\n",
      "Epoch [94/100], Loss: 0.0052, Accuracy: 99.65%\n",
      "Validation Loss: 2.0074, Validation Accuracy: 67.28%\n",
      "\n",
      "Epoch [95/100], Loss: 0.0053, Accuracy: 99.56%\n",
      "Validation Loss: 2.0171, Validation Accuracy: 67.89%\n",
      "\n",
      "Epoch [96/100], Loss: 0.0052, Accuracy: 99.56%\n",
      "Validation Loss: 1.9932, Validation Accuracy: 66.87%\n",
      "\n",
      "Epoch [97/100], Loss: 0.0053, Accuracy: 99.56%\n",
      "Validation Loss: 1.9480, Validation Accuracy: 67.68%\n",
      "\n",
      "Epoch [98/100], Loss: 0.0050, Accuracy: 99.69%\n",
      "Validation Loss: 2.0087, Validation Accuracy: 66.87%\n",
      "\n",
      "Epoch [99/100], Loss: 0.0047, Accuracy: 99.61%\n",
      "Validation Loss: 2.0185, Validation Accuracy: 67.68%\n",
      "\n",
      "Epoch [100/100], Loss: 0.0052, Accuracy: 99.56%\n",
      "Validation Loss: 2.0013, Validation Accuracy: 66.46%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16860\\2556072707.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('vgg16_best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating final evaluation metrics...\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           EA       0.79      0.89      0.84        80\n",
      "   EH_Complex       0.53      0.45      0.49        42\n",
      "    EH_Simple       0.77      0.79      0.78        78\n",
      "           EP       0.63      0.61      0.62        95\n",
      "NE_Follicular       0.69      0.69      0.69       107\n",
      "    NE_Luteal       0.83      0.79      0.81        90\n",
      "\n",
      "     accuracy                           0.72       492\n",
      "    macro avg       0.70      0.70      0.70       492\n",
      " weighted avg       0.72      0.72      0.72       492\n",
      "\n",
      "\n",
      "Confusion matrix has been saved as 'confusion_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset path\n",
    "data_dir = r'D:\\Engineering\\SEM7\\COMPUTER VISION\\proj_imp\\DATASET\\org_datahistopath_img_data'\n",
    "batch_size = 32\n",
    "img_size = 224  # VGG16 requires 224x224 input\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_data = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\n",
    "val_data = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load pre-trained VGG16\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Modify the classifier for our dataset\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, len(train_data.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\\n\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'vgg16_best_model.pth')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100)\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model.load_state_dict(torch.load('vgg16_best_model.pth'))\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "print(\"\\nGenerating final evaluation metrics...\")\n",
    "_, _, val_preds, val_labels = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=train_data.classes))\n",
    "\n",
    "# Generate and save confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plot_confusion_matrix(cm, train_data.classes)\n",
    "print(\"\\nConfusion matrix has been saved as 'confusion_matrix.png'\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'class_to_idx': train_data.class_to_idx\n",
    "}, 'vgg16_final_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
